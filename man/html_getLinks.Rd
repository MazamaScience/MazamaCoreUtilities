% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/html_getLinks.R
\name{html_getLinks}
\alias{html_getLinks}
\title{Find all links in an html page}
\usage{
html_getLinks(url = NULL)
}
\arguments{
\item{url}{URL or file path of an html page.}
}
\value{
A dataframe with \code{linkName} and \code{linkUrl} columns.
}
\description{
Parses an html page to extract all \code{<a href="...">...</a>}
links and return them in a dataframe where \code{linkName} is the human
readable name and \code{linkUrl} is the \code{href} portion.

This is especially useful for extracting data from an index page that shows
the contents of a web accessible directory.
}
\examples{
\donttest{
library(MazamaCoreUtils)

# US Census 2019 shapefiles
dataLinks <- html_getLinks("https://www2.census.gov/geo/tiger/GENZ2019/shp/")

dataLinks \%>\%
  dplyr::filter(stringr::str_detect(linkName, "us_county"))
}
}
